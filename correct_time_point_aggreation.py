import pandas as pd
import numpy as np
import os

def correct_time_point_aggreation():
    """Simplified version focusing on Transformer model with MEDIAN aggregation strategy"""
    
    print("=== Transformer Model with MEDIAN Aggregation Strategy ===")
    
    # æ£€æŸ¥å¿…è¦æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    required_files = [
        'Light_data.xlsx',
        'score.xlsx',
        'dataset/LIGHT_SMAP/LIGHT_SMAP_test.npy',
        'dataset/LIGHT_SMAP/LIGHT_SMAP_test_label.npy',
        'dataset/LIGHT_SMAP/LIGHT_SMAP_train.npy'
    ]
    
    missing_files = [f for f in required_files if not os.path.exists(f)]
    if missing_files:
        print(f"é”™è¯¯ï¼šç¼ºå°‘ä»¥ä¸‹å¿…è¦æ–‡ä»¶ï¼š")
        for f in missing_files:
            print(f"  - {f}")
        return
    
    # åŠ è½½æ•°æ®
    light_data = pd.read_excel('Light_data.xlsx')
    light_data['å·¥å†µæ—¶é—´'] = pd.to_datetime(light_data['å·¥å†µæ—¶é—´'])
    test_cutoff = pd.to_datetime('2022-07-01')
    test_data = light_data[light_data['å·¥å†µæ—¶é—´'] >= test_cutoff].copy()
    
    score_data = pd.read_excel('score.xlsx')
    score_data = score_data[~score_data['æ—¥æœŸ'].astype(str).str.contains('æ€»åˆ†', na=False)]
    score_data['æ—¥æœŸ'] = pd.to_datetime(score_data['æ—¥æœŸ'])
    
    print(f"æµ‹è¯•æ•°æ®æ¡æ•°: {len(test_data)}")
    print(f"è¯„åˆ†æ•°æ®æ¡æ•°: {len(score_data)}")
    
    # åŠ è½½Transformeræ¨¡å‹çš„å¼‚å¸¸åˆ†æ•°
    model_name = 'Transformer'
    test_result_path = 'Time-Series-Library/test_results/anomaly_detection_LIGHT_SMAP_Transformer_LIGHT_SMAP_ftM_sl100_ll48_pl0_dm128_nh8_el3_dl1_df128_expand2_dc4_fc1_ebtimeF_dtTrue_test_0'
    
    if not os.path.exists(test_result_path):
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°Transformeræ¨¡å‹çš„æµ‹è¯•ç»“æœç›®å½•: {test_result_path}")
        return
    
    anomaly_scores = np.load(f'{test_result_path}/anomaly_scores.npy')
    
    # é‡å¡‘ä¸ºçª—å£æ ¼å¼
    seq_len = 100
    expected_windows = len(test_data) - seq_len + 1
    reshaped = anomaly_scores.reshape(expected_windows, seq_len)
    
    print(f"çª—å£æ•°: {expected_windows}")
    print(f"åŸå§‹åˆ†æ•°å½¢çŠ¶: {anomaly_scores.shape}")
    print(f"é‡å¡‘åå½¢çŠ¶: {reshaped.shape}")
    
    # æ­¥éª¤1: æŒ‰æ—¶é—´ç‚¹é‡æ–°èšåˆ
    print("\næ­¥éª¤1: æŒ‰æ—¶é—´ç‚¹é‡æ–°èšåˆ...")
    time_point_scores = {}
    
    for window_idx in range(expected_windows):
        for pos_in_window in range(seq_len):
            actual_time_idx = window_idx + pos_in_window
            
            if actual_time_idx < len(test_data):
                score = reshaped[window_idx, pos_in_window]
                
                if actual_time_idx not in time_point_scores:
                    time_point_scores[actual_time_idx] = []
                time_point_scores[actual_time_idx].append(score)
    
    print(f"æ€»æ—¶é—´ç‚¹æ•°: {len(time_point_scores)}")
    
    # ä½¿ç”¨MEDIANç­–ç•¥
    print("\næ­¥éª¤2: ä½¿ç”¨MEDIANç­–ç•¥èšåˆæ¯ä¸ªæ—¶é—´ç‚¹çš„åˆ†æ•°...")
    aggregated_scores = {}
    
    for time_idx, scores in time_point_scores.items():
        aggregated_scores[time_idx] = np.median(scores)
    
    print(f"èšåˆåæ—¶é—´ç‚¹æ•°: {len(aggregated_scores)}")
    
    # æ˜¾ç¤ºå‰5ä¸ªæ—¶é—´ç‚¹çš„èšåˆç¤ºä¾‹
    print("\nå‰5ä¸ªæ—¶é—´ç‚¹çš„MEDIANèšåˆç¤ºä¾‹:")
    for i, (time_idx, final_score) in enumerate(list(aggregated_scores.items())[:5]):
        original_scores = time_point_scores[time_idx]
        time_point = test_data.iloc[time_idx]['å·¥å†µæ—¶é—´']
        print(f"  æ—¶é—´ç‚¹{time_idx} ({time_point}): {len(original_scores)}ä¸ªåˆ†æ•° -> MEDIAN {final_score:.6f}")
    
    # æ­¥éª¤3: ç¡®å®šæ—¶é—´ç‚¹çº§å¼‚å¸¸é˜ˆå€¼
    print("\næ­¥éª¤3: ä¼˜åŒ–å¼‚å¸¸é˜ˆå€¼...")
    all_scores = list(aggregated_scores.values())
    
    thresholds = [90, 95, 98, 99]
    best_result = None
    best_score = float('-inf')
    
    for threshold_percentile in thresholds:
        threshold = np.percentile(all_scores, threshold_percentile)
        
        # æ­¥éª¤4: åˆ¤æ–­æ¯ä¸ªæ—¶é—´ç‚¹æ˜¯å¦å¼‚å¸¸
        anomalous_time_points = []
        for time_idx, score in aggregated_scores.items():
            if score > threshold:
                time_point = test_data.iloc[time_idx]['å·¥å†µæ—¶é—´']
                anomalous_time_points.append((time_idx, time_point, score))
        
        # æ­¥éª¤5: æŒ‰æ—¥æœŸåˆ†ç»„ï¼Œåˆ¤æ–­æ¯å¤©çš„å¼‚å¸¸æ—¶é—´ç‚¹æ•°é‡
        daily_anomaly_counts = {}
        
        for time_idx, time_point, score in anomalous_time_points:
            date = time_point.date()
            if date not in daily_anomaly_counts:
                daily_anomaly_counts[date] = []
            daily_anomaly_counts[date].append((time_point, score))
        
        # æ­¥éª¤6: ä½¿ç”¨é˜ˆå€¼åˆ¤æ–­æ—¥çº§å¼‚å¸¸
        daily_labels = {}
        min_anomaly_points = 8
        
        # å…ˆæŠŠæ‰€æœ‰æ—¥æœŸæ ‡è®°ä¸ºæ­£å¸¸
        for _, row in score_data.iterrows():
            date = row['æ—¥æœŸ'].date()
            daily_labels[date] = 0
        
        # ç„¶åæ ‡è®°å¼‚å¸¸æ—¥æœŸ
        for date, anomaly_points in daily_anomaly_counts.items():
            if len(anomaly_points) > min_anomaly_points:
                daily_labels[date] = 1
        
        anomaly_days = sum(daily_labels.values())
        
        # æ­¥éª¤7: è®¡ç®—æœ€ç»ˆè¯„åˆ†
        result_df = score_data.copy()
        result_df['ç®—æ³•æ ‡ç­¾'] = np.nan
        
        # å¡«å…¥é¢„æµ‹æ ‡ç­¾
        for idx, row in result_df.iterrows():
            date = row['æ—¥æœŸ'].date()
            if date in daily_labels:
                result_df.at[idx, 'ç®—æ³•æ ‡ç­¾'] = daily_labels[date]
        
        # è®¡ç®—è¯„åˆ†
        total_score = 0
        false_positives = 0
        false_negatives = 0
        true_positives = 0
        true_negatives = 0
        
        for _, row in result_df.iterrows():
            true_label = row['çœŸå®æ ‡ç­¾']
            pred_label = row['ç®—æ³•æ ‡ç­¾']
            
            if pd.notna(true_label) and pd.notna(pred_label):
                if true_label == 0 and pred_label == 1:  # è¯¯æŠ¥
                    total_score -= 1
                    false_positives += 1
                elif true_label == 1 and pred_label == 0:  # æ¼æŠ¥
                    total_score -= 20
                    false_negatives += 1
                elif true_label == 1 and pred_label == 1:  # æ­£ç¡®æ£€æµ‹å¼‚å¸¸
                    true_positives += 1
                elif true_label == 0 and pred_label == 0:  # æ­£ç¡®æ£€æµ‹æ­£å¸¸
                    true_negatives += 1
        
        if total_score > best_score:
            best_score = total_score
            best_result = {
                'threshold_percentile': threshold_percentile,
                'threshold_value': threshold,
                'score': total_score,
                'result_df': result_df.copy(),
                'true_positives': true_positives,
                'false_positives': false_positives,
                'false_negatives': false_negatives,
                'anomaly_days': anomaly_days,
                'anomalous_time_points': anomalous_time_points
            }
    
    # è¾“å‡ºæœ€ç»ˆç»“æœ
    print(f"\nğŸ† Transformer + MEDIANç­–ç•¥æœ€ä½³ç»“æœ:")
    print(f"  æœ€ä½³é˜ˆå€¼: ç¬¬{best_result['threshold_percentile']}ç™¾åˆ†ä½æ•°")
    print(f"  æœ€ä½³å¾—åˆ†: {best_result['score']}")
    print(f"  æ£€æµ‹å¼‚å¸¸: {best_result['true_positives']}/5")
    print(f"  è¯¯æŠ¥: {best_result['false_positives']}æ¬¡")
    print(f"  æ¼æŠ¥: {best_result['false_negatives']}æ¬¡")
    print(f"  å¼‚å¸¸å¤©æ•°: {best_result['anomaly_days']}å¤©")
    
    # ä¿å­˜æœ€ä½³ç»“æœ
    output_file = 'score_correct_Transformer_median.xlsx'
    best_result['result_df'].to_excel(output_file, index=False)
    print(f"\næœ€ä½³ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # è¾“å‡ºå¼‚å¸¸æ—¥æœŸè¯¦æƒ…
    print(f"\nğŸ“Š å¼‚å¸¸æ—¥æœŸè¯¦æƒ…:")
    anomalous_dates = []
    for _, row in best_result['result_df'].iterrows():
        if row['ç®—æ³•æ ‡ç­¾'] == 1:
            anomalous_dates.append(row['æ—¥æœŸ'].strftime('%Y-%m-%d'))
    
    if anomalous_dates:
        print(f"  æ£€æµ‹åˆ°å¼‚å¸¸çš„æ—¥æœŸ: {', '.join(anomalous_dates)}")
    else:
        print("  æœªæ£€æµ‹åˆ°å¼‚å¸¸æ—¥æœŸ")
    
    return best_result

if __name__ == "__main__":
    correct_time_point_aggreation()